{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pipipu/miniconda3/envs/DLML/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Type_Embedding(torch.nn.Module):\n",
    "    def __init__(self, seq_len, hidden_dim, output_dim):\n",
    "        super(Type_Embedding, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.MLP1 = nn.Linear(self.seq_len, self.hidden_dim)\n",
    "        self.ac = nn.GELU()\n",
    "        self.MLP2 = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.MLP1(x)\n",
    "        x = self.ac(x)\n",
    "        x = self.MLP2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Separate_Embedding(torch.nn.Module):\n",
    "    def __init__(self, seq_len, hidden_dim, output_dim):\n",
    "        super(Separate_Embedding, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.linear_layers = nn.ModuleList([\n",
    "            Type_Embedding(self.seq_len, self.hidden_dim,self.output_dim) for _ in range(10)\n",
    "        ])\n",
    "       \n",
    "    def forward(self, x):\n",
    "        tensor_list = torch.split(x, 1, dim=0)\n",
    "        processed_tensors = []\n",
    "\n",
    "        for i, tensor in enumerate(tensor_list):\n",
    "            linear_layer = self.linear_layers[i]\n",
    "            processed_tensor = linear_layer(tensor)\n",
    "            processed_tensors.append(processed_tensor)\n",
    "        \n",
    "        combined_tensor = torch.cat(processed_tensors, dim=0)\n",
    "        return combined_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 133, 8])\n",
      "torch.Size([8, 133, 4])\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of Separate_Embedding\n",
    "separate_embedding = Separate_Embedding(seq_len=8, hidden_dim=16, output_dim=4)\n",
    "\n",
    "# Example input tensor\n",
    "input_tensor = torch.randn(8, 133, 8)\n",
    "\n",
    "# Apply the separate embedding\n",
    "output_tensor = separate_embedding(input_tensor)\n",
    "\n",
    "# Print the output tensor\n",
    "print(output_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Adj_Embedding(torch.nn.Module):\n",
    "    def __init__(self, adj_matrix, hidden_dim_1, hidden_dim_2, output_dim, k):\n",
    "        super(Adj_Embedding, self).__init__()\n",
    "        self.adj_matrix = adj_matrix\n",
    "        self.input_dim = self.adj_matrix.shape[0]\n",
    "        self.hidden_dim_1 = hidden_dim_1\n",
    "        self.hidden_dim_2 = hidden_dim_2\n",
    "        self.output_dim = output_dim\n",
    "        self.k = k\n",
    "\n",
    "        self.MLP1_d1 = nn.Linear(self.input_dim, self.hidden_dim_1)\n",
    "        self.ac1_d1 = nn.GELU()\n",
    "        self.MLP2_d1 = nn.Linear(self.hidden_dim_1, self.output_dim)\n",
    "\n",
    "        self.MLP1_d2 = nn.Linear(self.input_dim, self.hidden_dim_2)\n",
    "        self.ac1_d2 = nn.GELU()\n",
    "        self.MLP2_d2 = nn.Linear(self.hidden_dim_2, self.output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.MLP1_d1(x)\n",
    "        x = self.ac1_d1(x)\n",
    "        x = x.reshape(x.shape[1], x.shape[0])\n",
    "        x = self.MLP1_d2(x)\n",
    "        x = self.ac1_d2(x)\n",
    "        x = self.MLP2_d2(x)\n",
    "        x = x.reshape(x.shape[1], x.shape[0])\n",
    "        x = self.MLP2_d1(x)\n",
    "        \n",
    "        softmax_output = torch.nn.functional.softmax(x, dim=0)\n",
    "        topk_values, topk_indices = torch.topk(softmax_output, self.k, dim=0)\n",
    "        masked_output = torch.where(softmax_output < topk_values[-1], torch.tensor(0.0), softmax_output)\n",
    "\n",
    "        return masked_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 10])\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of Separate_Embedding\n",
    "adj = torch.randn(10,10)\n",
    "hidden_dim_1 = 32\n",
    "hidden_dim_2 = 32 \n",
    "output_dim = 10\n",
    "k = 3\n",
    "separate_embedding = Adj_Embedding(adj,hidden_dim_1,hidden_dim_2,output_dim,k)\n",
    "\n",
    "# Example input tensor\n",
    "\n",
    "# Apply the separate embedding\n",
    "output_tensor = separate_embedding(adj)\n",
    "\n",
    "# Print the output tensor\n",
    "print(output_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = np.load('/Users/pipipu/Desktop/HAGEN/HAGEN-code/crime-data/sensor_graph/adj_mx_la.pkl',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(torch.nn.Module):\n",
    "    def __init__(self, seq_len, hidden_dim, output_dim):\n",
    "        super(Linear, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.MLP1 = nn.Linear(self.seq_len, self.hidden_dim)\n",
    "        self.ac = nn.GELU()\n",
    "        self.MLP2 = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.MLP1(x)\n",
    "        x = self.ac(x)\n",
    "        x = self.MLP2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Framework(torch.nn.Module):\n",
    "    def __init__(self, seq_len, hidden_dim_type, output_dim_type, adj_matrix, hidden_dim_1, hidden_dim_2, output_dim_adj, k, input_dim_main, hidden_dim_main,output_dim_main):\n",
    "        super(Framework, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.hidden_dim_type = hidden_dim_type\n",
    "        self.output_dim_type = output_dim_type\n",
    "\n",
    "        self.embedding = Separate_Embedding(self.seq_len, self.hidden_dim_type, self.output_dim_type)\n",
    "\n",
    "        self.adj_matrix = adj_matrix\n",
    "        self.matrix_dim = adj_matrix.shape[0]\n",
    "        self.hidden_dim_1_adj = hidden_dim_1\n",
    "        self.hidden_dim_2_adj = hidden_dim_2\n",
    "        self.output_dim_adj = output_dim_adj\n",
    "        self.k = k\n",
    "\n",
    "        self.adj_embedding = Adj_Embedding(self.adj_matrix, self.hidden_dim_1_adj, self.hidden_dim_2_adj, self.output_dim_adj, self.k)\n",
    "\n",
    "        self.input_dim_main = input_dim_main\n",
    "        self.hidden_dim_main = hidden_dim_main\n",
    "        self.output_dim_main = output_dim_main\n",
    "        self.main = Linear(self.input_dim_main,self.hidden_dim_main,self.output_dim_main)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        x = x.permute(0,3,2,1)\n",
    "        self.adj = self.adj_embedding(self.adj_matrix)\n",
    "        \n",
    "        x = self.main(x)\n",
    "        x = x.permute(0,3,2,1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = np.load(\"/Users/pipipu/Desktop/HAGEN/HAGEN-code/crime-data/CRIME-LA/8/train.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.from_numpy(X_data['x'])\n",
    "y = torch.from_numpy(X_data['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "def train(model, train_loader, num_epochs, learning_rate):\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Iterate over the training dataset\n",
    "        for inputs, labels in train_loader:\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update the running loss\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Print the average loss for the epoch\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss}\")\n",
    "\n",
    "    print(\"Training finished!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 8\n",
    "hidden_dim_type = 32\n",
    "output_dim_type = 32\n",
    "adj_matrix =  np.load('/Users/pipipu/Desktop/HAGEN/HAGEN-code/crime-data/sensor_graph/adj_mx_la.pkl',allow_pickle = True)[2]\n",
    "hidden_dim_1 = 32\n",
    "hidden_dim_2 = 32\n",
    "output_dim_adj = 113\n",
    "k = 3\n",
    "input_dim_main = output_dim_type\n",
    "hidden_dim_main = 64\n",
    "output_dim_main = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criterion = nn.MSELoss()\n",
    "#model = Framework(seq_len, hidden_dim_type, output_dim_type, adj_matrix, hidden_dim_1, hidden_dim_2, output_dim_adj, k, input_dim_main, hidden_dim_main,output_dim_main)\n",
    "model = Linear(8,32,1)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_loss(y_true, y_pred, quantile):\n",
    "    error = y_true - y_pred\n",
    "    loss = torch.max(quantile * error, (quantile - 1) * error)\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Loss: 0.18466472983149296\n",
      "Epoch 2/30, Loss: 0.09025995660454827\n",
      "Epoch 3/30, Loss: 0.08488789093030603\n",
      "Epoch 4/30, Loss: 0.08301902384323276\n",
      "Epoch 5/30, Loss: 0.0813967119836051\n",
      "Epoch 6/30, Loss: 0.07986565988056392\n",
      "Epoch 7/30, Loss: 0.07842189509477727\n",
      "Epoch 8/30, Loss: 0.07705856925575234\n",
      "Epoch 9/30, Loss: 0.07576786133897331\n",
      "Epoch 10/30, Loss: 0.07453017846676477\n",
      "Epoch 11/30, Loss: 0.07336567940921891\n",
      "Epoch 12/30, Loss: 0.07225903314760518\n",
      "Epoch 13/30, Loss: 0.07125749467454605\n",
      "Epoch 14/30, Loss: 0.07048016603609816\n",
      "Epoch 15/30, Loss: 0.07008490830613616\n",
      "Epoch 16/30, Loss: 0.06998084875143741\n",
      "Epoch 17/30, Loss: 0.06992199143292038\n",
      "Epoch 18/30, Loss: 0.06988272291032736\n",
      "Epoch 19/30, Loss: 0.06984810425354465\n",
      "Epoch 20/30, Loss: 0.06981583465417554\n",
      "Epoch 21/30, Loss: 0.06978576670941875\n",
      "Epoch 22/30, Loss: 0.06975699144875555\n",
      "Epoch 23/30, Loss: 0.06972914543707467\n",
      "Epoch 24/30, Loss: 0.06970172416985024\n",
      "Epoch 25/30, Loss: 0.06967486567016132\n",
      "Epoch 26/30, Loss: 0.06964845767279093\n",
      "Epoch 27/30, Loss: 0.06962250244052377\n",
      "Epoch 28/30, Loss: 0.0695971113776021\n",
      "Epoch 29/30, Loss: 0.06957248717160264\n",
      "Epoch 30/30, Loss: 0.06954877360149753\n"
     ]
    }
   ],
   "source": [
    "model = Linear(8,32,1)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "model = model.double()\n",
    "num_epochs = 30\n",
    "quantile = 0.9\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0\n",
    "    for i in range(X.shape[0]):\n",
    "        inputs = X[i,:,:,:]\n",
    "        inputs = inputs.permute(2,1,0)\n",
    "        labels = y[i,:,:,:]\n",
    "        labels = labels.permute(2,1,0)\n",
    "        outputs = model(inputs)\n",
    "        loss = quantile_loss(labels,outputs,quantile)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "            # Update the running loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Print the average loss for the epoch\n",
    "    epoch_loss = running_loss / X.shape[0]\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss}\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0001, dtype=torch.float64, grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = X[0,:,:,:]\n",
    "Y = y[0,:,:,:]\n",
    "x = x.permute(2,1,0)\n",
    "Y = Y.permute(2,1,0)\n",
    "outputs = model(x)\n",
    "outputs.shape\n",
    "print(outputs[0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., dtype=torch.float64)"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Loss: 0.04269981439246525\n",
      "Epoch 2/30, Loss: 0.03892350016362153\n",
      "Epoch 3/30, Loss: 0.03648743339206173\n",
      "Epoch 4/30, Loss: 0.035305437111997906\n",
      "Epoch 5/30, Loss: 0.03466622829106906\n",
      "Epoch 6/30, Loss: 0.034305920620383\n",
      "Epoch 7/30, Loss: 0.034089073234392274\n",
      "Epoch 8/30, Loss: 0.033927833704458586\n",
      "Epoch 9/30, Loss: 0.03378522977036841\n",
      "Epoch 10/30, Loss: 0.03365537963917146\n",
      "Epoch 11/30, Loss: 0.03353747728682481\n",
      "Epoch 12/30, Loss: 0.03343538873209557\n",
      "Epoch 13/30, Loss: 0.033339480709038936\n",
      "Epoch 14/30, Loss: 0.03324774962992209\n",
      "Epoch 15/30, Loss: 0.033162286464586105\n",
      "Epoch 16/30, Loss: 0.03307993669673301\n",
      "Epoch 17/30, Loss: 0.03300196616719869\n",
      "Epoch 18/30, Loss: 0.032928831454653855\n",
      "Epoch 19/30, Loss: 0.03286521458165528\n",
      "Epoch 20/30, Loss: 0.03281056275442542\n",
      "Epoch 21/30, Loss: 0.03275422962913563\n",
      "Epoch 22/30, Loss: 0.03270204058628107\n",
      "Epoch 23/30, Loss: 0.032652281283085595\n",
      "Epoch 24/30, Loss: 0.03260717415071609\n",
      "Epoch 25/30, Loss: 0.032563185654584804\n",
      "Epoch 26/30, Loss: 0.032524206042039136\n",
      "Epoch 27/30, Loss: 0.032492918779659534\n",
      "Epoch 28/30, Loss: 0.03245895165142479\n",
      "Epoch 29/30, Loss: 0.03242350045487962\n",
      "Epoch 30/30, Loss: 0.03239317559876916\n"
     ]
    }
   ],
   "source": [
    "model = Linear(8,32,1)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "model = model.double()\n",
    "num_epochs = 30\n",
    "quantile = 0.1\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0\n",
    "    for i in range(X.shape[0]):\n",
    "        inputs = X[i,:,:,:]\n",
    "        inputs = inputs.permute(2,1,0)\n",
    "        labels = y[i,:,:,:]\n",
    "        labels = labels.permute(2,1,0)\n",
    "        outputs = model(inputs)\n",
    "        loss = quantile_loss(labels,outputs,quantile)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "            # Update the running loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Print the average loss for the epoch\n",
    "    epoch_loss = running_loss / X.shape[0]\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss}\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 113, 1])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 8 is out of bounds for dimension 0 with size 8",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[302], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m outputs \u001b[39m=\u001b[39m model(x)\n\u001b[1;32m      6\u001b[0m \u001b[39mprint\u001b[39m(outputs\u001b[39m.\u001b[39mshape)\n\u001b[0;32m----> 7\u001b[0m \u001b[39mprint\u001b[39m(outputs[\u001b[39m8\u001b[39;49m,\u001b[39m0\u001b[39;49m,\u001b[39m0\u001b[39;49m])\n",
      "\u001b[0;31mIndexError\u001b[0m: index 8 is out of bounds for dimension 0 with size 8"
     ]
    }
   ],
   "source": [
    "x = X[0,:,:,:]\n",
    "Y = y[0,:,:,:]\n",
    "x = x.permute(2,1,0)\n",
    "Y = Y.permute(2,1,0)\n",
    "outputs = model(x)\n",
    "print(outputs.shape)\n",
    "print(outputs[8,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(Y[7,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = torch.from_numpy(np.array([[1,2,3],[4,5,6]]))\n",
    "y_true = torch.from_numpy(np.array([[1,1,1],[1,1,1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Stochastic gradient MCMC implementation based on Diffusion Net\n",
    "\"\"\"\n",
    "\n",
    "import sys, copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class Sampler:\n",
    "    def __init__(self, net, criterion, momentum=0.0, lr=0.1, wdecay=0.0, T=0.05, total=50000, L2_weight = 0.0):\n",
    "        self.net = net\n",
    "        self.eta = lr\n",
    "        self.momentum = momentum\n",
    "        self.T = T\n",
    "        self.wdecay = wdecay\n",
    "        self.V = 0.1\n",
    "        self.criterion = criterion\n",
    "        self.total = total\n",
    "        self.L2_weight = L2_weight\n",
    "\n",
    "        print(\"Learning rate: \")\n",
    "        print(self.eta)\n",
    "        print(\"Noise std: \")\n",
    "        print(self.scale)\n",
    "        print(\"L2 penalty:\")\n",
    "        print(self.L2_weight)\n",
    "\n",
    "    \n",
    "    def backprop(self, x, y, batches_seen):\n",
    "        self.net.zero_grad()\n",
    "        \"\"\" convert mean loss to sum losses \"\"\"\n",
    "        output = self.net(x, y, batches_seen)\n",
    "        loss = self.criterion(y, output) * self.total\n",
    "        loss.backward()\n",
    "        return loss \n",
    "    \n",
    "    # SGD without momentum\n",
    "    def step(self, x, y, batches_seen):\n",
    "        loss = self.backprop(x, y, batches_seen)\n",
    "        for i, param in enumerate(self.net.parameters()):\n",
    "            proposal = torch.cuda.FloatTensor(param.data.size()).normal_().mul((self.eta*2.0)**0.5)\n",
    "            proposal.add_(-0.5*self.scale, param.data)\n",
    "            grads = param.grad.data\n",
    "            if self.wdecay != 0:\n",
    "                grads.add_(self.wdecay, param.data)\n",
    "            # self.velocity[i].mul_(self.momentum).add_(-self.eta, grads).add_(proposal)\n",
    "            # param.data.add_(self.velocity[i])\n",
    "            param.data.add_(-self.eta, grads).add_(proposal)\n",
    "        return loss.data.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train(base_lr,\n",
    "               steps, patience=50, epochs=100, lr_decay_ratio=0.1, log_every=1, save_model=1,\n",
    "               test_every_n_epochs=10, epsilon=1e-8):\n",
    "        # steps is used in learning rate - will see if need to use it?\n",
    "        min_val_loss = float('inf')\n",
    "        wait = 0\n",
    "        # Initially setup some parameters\n",
    "        sampler = None\n",
    "\n",
    "      \n",
    "        # this will fail if model is loaded with a changed batch_size\n",
    "        num_batches = _data['train_loader'].num_batch\n",
    "       \n",
    "\n",
    "        batches_seen = num_batches * _epoch_num\n",
    "        base_lr *= 0.1\n",
    "\n",
    "        for epoch_num in range(_epoch_num, epochs):\n",
    "\n",
    "            dcrnn_model = dcrnn_model.train()\n",
    "\n",
    "            train_iterator = _data['train_loader'].get_iterator()\n",
    "            losses = []\n",
    "\n",
    "\n",
    "\n",
    "            for _, (x, y) in enumerate(train_iterator):\n",
    "                x, y = _prepare_data(x, y)\n",
    "                \n",
    "                if batches_seen == 0: \n",
    "                    sampler = Sampler(dcrnn_model, _compute_loss, momentum=0.0, lr=base_lr, wdecay=0.0, T=0.01, total=23872, L2_weight = 0.3)\n",
    "\n",
    "                loss = sampler.step(x, y, batches_seen)\n",
    "\n",
    "                _logger.debug(loss / 23872.0)\n",
    "\n",
    "                losses.append(loss / 23872.0)\n",
    "\n",
    "                batches_seen += 1\n",
    "\n",
    "                # gradient clipping - this does it in place\n",
    "                torch.nn.utils.clip_grad_norm_(dcrnn_model.parameters(), max_grad_norm)\n",
    "\n",
    "            self._logger.info(\"epoch complete\")\n",
    "            # lr_scheduler.step()\n",
    "            \"\"\" Anneaing \"\"\"\n",
    "            if epoch_num > (0.04 * self.sn) and self.lr_anneal <= 1.:\n",
    "                sampler.eta *= self.lr_anneal\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "            # self._writer.add_scalar('training loss',\n",
    "            #                         np.mean(losses),\n",
    "            #                         batches_seen)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node Labels:\n",
      "Node 0: Label 1\n",
      "Node 1: Label 1\n",
      "Node 2: Label 0\n",
      "Node 3: Label 0\n",
      "Node 4: Label 2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "# Example adjacency matrix representing the graph\n",
    "adjacency_matrix = np.array([[0, 1, 1, 0, 0],\n",
    "                            [1, 0, 1, 0, 0],\n",
    "                            [1, 1, 0, 1, 0],\n",
    "                            [0, 0, 1, 0, 1],\n",
    "                            [0, 0, 0, 1, 0]])\n",
    "\n",
    "# Number of desired partitions\n",
    "n_partitions = 3\n",
    "\n",
    "# Perform spectral clustering\n",
    "sc = SpectralClustering(n_clusters=n_partitions, affinity='precomputed', random_state=42)\n",
    "labels = sc.fit_predict(adjacency_matrix)\n",
    "\n",
    "# Print the assigned labels for each node\n",
    "print(\"Node Labels:\")\n",
    "for i, label in enumerate(labels):\n",
    "    print(f\"Node {i}: Label {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-Adjacency Matrices:\n",
      "Sub-Adjacency Matrix 1:\n",
      "[[1.         1.         0.374108   ... 0.21471389 0.9955341  0.7400604 ]\n",
      " [1.         1.         0.5280353  ... 0.24442069 0.8320746  0.9150456 ]\n",
      " [0.374108   0.5280353  1.         ... 0.8042993  0.13124737 0.8469326 ]\n",
      " ...\n",
      " [0.21471389 0.24442069 0.8042993  ... 1.         0.         0.4348732 ]\n",
      " [0.9955341  0.8320746  0.13124737 ... 0.         1.         0.3648845 ]\n",
      " [0.7400604  0.9150456  0.8469326  ... 0.4348732  0.3648845  1.        ]]\n",
      "Sub-Adjacency Matrix 2:\n",
      "[[1.         0.9928625  0.90650445 ... 0.3050112  0.57730895 0.33258843]\n",
      " [0.9928625  1.         0.4299091  ... 0.         0.16850382 0.4319986 ]\n",
      " [0.90650445 0.4299091  1.         ... 0.76377624 1.         0.        ]\n",
      " ...\n",
      " [0.3050112  0.         0.76377624 ... 1.         1.         0.        ]\n",
      " [0.57730895 0.16850382 1.         ... 1.         1.         0.        ]\n",
      " [0.33258843 0.4319986  0.         ... 0.         0.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "# Example adjacency matrix representing the graph\n",
    "adjacency_matrix = np.array([[1, 0.5, 0, 0, 0],\n",
    "                            [1, 1, 1, 0, 0],\n",
    "                            [0, 1, 1, 1, 0],\n",
    "                            [0, 0, 1, 1, 1],\n",
    "                            [0, 0, 0, 1, 1]])\n",
    "adjacency_matrix = np.load('/Users/pipipu/Desktop/HAGEN/HAGEN-code/crime-data/sensor_graph/adj_mx_la.pkl',allow_pickle = True)[2]\n",
    "# Number of desired partitions\n",
    "n_partitions = 2\n",
    "#data = np.array([[1,2,3,4,5],[1,2,3,4,5]])\n",
    "# Perform spectral clustering\n",
    "sc = SpectralClustering(n_clusters=n_partitions, affinity='precomputed', random_state=42)\n",
    "labels = sc.fit_predict(adjacency_matrix)\n",
    "\n",
    "\n",
    "# Create sub-adjacency matrices\n",
    "sub_adjacency_matrices = []\n",
    "data_matrices = []\n",
    "\n",
    "for label in np.unique(labels):\n",
    "    \n",
    "    sub_adjacency_matrix = adjacency_matrix[labels == label][:, labels == label]\n",
    "    sub_adjacency_matrices.append(sub_adjacency_matrix)\n",
    "    #data_matrix = data[:,labels == label]\n",
    "    #data_matrices.append(data_matrix)\n",
    "    \n",
    "# Print the sub-adjacency matrices\n",
    "print(\"Sub-Adjacency Matrices:\")\n",
    "for i, sub_adjacency_matrix in enumerate(sub_adjacency_matrices):\n",
    "    print(f\"Sub-Adjacency Matrix {i+1}:\\n{sub_adjacency_matrix}\")\n",
    "\n",
    "#print(data_matrices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_partition(data,adjacency_matrix,n_partitions):\n",
    "    sc = SpectralClustering(n_clusters=n_partitions, affinity='precomputed', random_state=42)\n",
    "    labels = sc.fit_predict(adjacency_matrix)\n",
    "    sub_adjacency_matrices = []\n",
    "    data_matrices = []\n",
    "    for label in np.unique(labels):\n",
    "        sub_adjacency_matrix = adjacency_matrix[labels == label][:, labels == label]\n",
    "        print(adjacency_matrix[labels == label])\n",
    "        sub_adjacency_matrices.append(sub_adjacency_matrix)\n",
    "        data_matrix = data[:,labels == label]\n",
    "        data_matrices.append(data_matrix)\n",
    "    \n",
    "    seq_len,num_nodes = data.shape\n",
    "    \n",
    "    return sub_adjacency_matrices, data_matrices, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 1. 1.]]\n",
      "[[1.  0.5 0.  0.  0. ]]\n",
      "[[1. 1. 1. 0. 0.]\n",
      " [0. 1. 1. 1. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pipipu/miniconda3/envs/DLML/lib/python3.9/site-packages/sklearn/manifold/_spectral_embedding.py:248: UserWarning: Array is not symmetric, and will be converted to symmetric by average with its transpose.\n",
      "  adjacency = check_symmetric(adjacency)\n"
     ]
    }
   ],
   "source": [
    "adjacency_matrix = np.array([[1, 0.5, 0, 0, 0],\n",
    "                            [1, 1, 1, 0, 0],\n",
    "                            [0, 1, 1, 1, 0],\n",
    "                            [0, 0, 1, 1, 1],\n",
    "                            [0, 0, 0, 1, 1]])\n",
    "\n",
    "data = np.array([[1,2,3,4,5],[1,2,3,4,5]])\n",
    "\n",
    "n_partitions = 3\n",
    "\n",
    "sub_adjacency_matrices, data_matrices, labels = matrix_partition(data,adjacency_matrix,n_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_reconstruct(labels,data_matrices,seq_len,num_nodes):\n",
    "\n",
    "    unique_labels = np.unique(labels)\n",
    "    k = len(unique_labels)\n",
    "\n",
    "    n = seq_len\n",
    "    m = num_nodes\n",
    "\n",
    "    recovered_matrix = np.zeros((n, m))\n",
    "    for i, small_matrix in enumerate(data_matrices):\n",
    "        indices = np.where(labels == unique_labels[i])[0]\n",
    "        recovered_matrix[:,indices] = small_matrix\n",
    "        \n",
    "    return recovered_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 3., 4., 5.],\n",
       "       [1., 2., 3., 4., 5.]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_reconstruct(labels, data_matrices, 2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(data_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[1, 2, 3]]), array([[ 4,  5,  6],\n",
      "       [10, 11, 12]]), array([[7, 8, 9]])]\n",
      "Smaller Matrices:\n",
      "Small Matrix 1:\n",
      "[[1 2 3]]\n",
      "Small Matrix 2:\n",
      "[[ 4  5  6]\n",
      " [10 11 12]]\n",
      "Small Matrix 3:\n",
      "[[7 8 9]]\n",
      "[0]\n",
      "[1 3]\n",
      "[2]\n",
      "\n",
      "Recovered Matrix:\n",
      "[[ 1.  2.  3.]\n",
      " [ 4.  5.  6.]\n",
      " [ 7.  8.  9.]\n",
      " [10. 11. 12.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example matrix and labels\n",
    "matrix = np.array([[1, 2, 3],\n",
    "                   [4, 5, 6],\n",
    "                   [7, 8, 9],\n",
    "                   [10, 11, 12]])\n",
    "labels = np.array([0, 1, 2, 1])\n",
    "\n",
    "# Determine the unique labels and the number of rows and columns in the matrix\n",
    "unique_labels = np.unique(labels)\n",
    "k = len(unique_labels)\n",
    "n, m = matrix.shape\n",
    "\n",
    "# Separate the big matrix into smaller matrices based on labels\n",
    "small_matrices = []\n",
    "for label in unique_labels:\n",
    "    small_matrix = matrix[labels == label]\n",
    "    small_matrices.append(small_matrix)\n",
    "\n",
    "print(small_matrices)\n",
    "# Print the smaller matrices\n",
    "print(\"Smaller Matrices:\")\n",
    "for i, small_matrix in enumerate(small_matrices):\n",
    "    print(f\"Small Matrix {i+1}:\\n{small_matrix}\")\n",
    "\n",
    "# Recover the original matrix\n",
    "recovered_matrix = np.zeros((n, m))\n",
    "for i, small_matrix in enumerate(small_matrices):\n",
    "    \n",
    "    indices = np.where(labels == unique_labels[i])[0]\n",
    "    print(indices)\n",
    "    recovered_matrix[indices] = small_matrix\n",
    "\n",
    "print(\"\\nRecovered Matrix:\")\n",
    "print(recovered_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_partition()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
